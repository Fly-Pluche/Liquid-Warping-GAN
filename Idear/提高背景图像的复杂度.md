

# 思想

<img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210803153537362.png" alt="image-20210803153537362" style="zoom:50%;" />

任谁看到这么模糊的背景都会想着提升效果。。。



# 提高背景复杂度

### *V1：*输入两次恢复网络，后面的mask用腐蚀处理

这个网络使用的是InpaintSANet，输入是污染图以及mask，

<img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210805122317142.png" alt="image-20210805122317142" style="zoom: 50%;" />

<img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210803153537362.png" alt="image-20210803153537362" style="zoom:50%;" />

<img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210803211630734.png" alt="image-20210803211630734" style="zoom: 33%;" /><img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210803211643953.png" alt="image-20210803211643953" style="zoom: 33%;" /><img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210803212729284.png" alt="image-20210803212729284" style="zoom: 33%;" />

当然，也做了不同的腐蚀次数已经多次使用恢复网络的处理，这里就不一一体现

我开始怀疑网络的性能。

看这篇Gan里面，提到过自己练了这个背景恢复网络。

```python
src_info['bg'] = self.bgnet(img, masks=body_mask, only_x=True)

body_mask=body_mask*255
body_mask=np.array(body_mask.cpu())
body_mask=body_mask.squeeze(0).squeeze(0)
body_mask = cv2.erode(body_mask, None, iterations=7)
a=cv2.erode(body_mask, None, iterations=5)
a=a.astype(np.uint8)
cv2.imwrite('腐蚀1.png',a)

body_mask=torch.from_numpy(body_mask/255)
body_mask=body_mask.unsqueeze(0).unsqueeze(0).cuda()
src_info['bg'] = self.bgnet(src_info['bg'], masks=body_mask,only_x=True)

body_mask=body_mask*255
body_mask=np.array(body_mask.cpu())
body_mask=body_mask.squeeze(0).squeeze(0)
body_mask = cv2.erode(body_mask, None, iterations=7)
a=cv2.erode(body_mask, None, iterations=5)
a=a.astype(np.uint8)
cv2.imwrite('腐蚀2.png',a)

body_mask=torch.from_numpy(body_mask/255)
body_mask=body_mask.unsqueeze(0).unsqueeze(0).cuda()
src_info['bg'] = self.bgnet(src_info['bg'], masks=body_mask,only_x=True)
```



### *V2:*将图片裁剪成四块，分别输入，然后拼合

我看恢复网络论文中效果挺好的。可能是缺失的部分在图片中占有的比例太大了。

<kbd>D:\workspace\impersonator-master\models\imitator.py</kbd>

```python
# Origin
src_info['bg'] = self.bgnet(img, masks=body_mask, only_x=True)

# Myself
img_1=img[:,:,:128,:128]
img_2=img[:,:,128:,128:]
img_3=img[:,:,:128,128:]
img_4=img[:,:,128:,:128]

body_mask_1=body_mask[:,:,:128,:128]
body_mask_2=body_mask[:,:,128:,128:]
body_mask_3=body_mask[:,:,:128,128:]
body_mask_4=body_mask[:,:,128:,:128]

img_1= self.bgnet(img_1, masks=body_mask_1, only_x=True)
img_2 = self.bgnet(img_2, masks=body_mask_2, only_x=True)
img_3 = self.bgnet(img_3, masks=body_mask_3, only_x=True)
img_4 = self.bgnet(img_4, masks=body_mask_4, only_x=True)

img_5=torch.cat((img_1,img_3),3)
img_6=torch.cat((img_4,img_2),3)

src_info['bg']=torch.cat((img_5,img_6),2)
```

<img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210805130424705.png" alt="image-20210805130424705" style="zoom:33%;" />

<img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210803153537362.png" alt="image-20210803153537362" style="zoom: 33%;" /><img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210805154803058.png" alt="image-20210805154803058" style="zoom: 33%;" />

<img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210805165303265.png" alt="image-20210805165303265" style="zoom:33%;" /><img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210805165324821.png" alt="image-20210805165324821" style="zoom:33%;" />

我直接好家伙┭┮﹏┭┮

在边界的信息缺失，弥补出的图像会有黄色边界。而且，我觉得网络也不太行，该模糊还是模糊。。。

### *V3:*

改进网络

1. 将mask 门控通过门控注意改

V3.1.1：

```python
原来：
x = self.activation(x) * self.gated(mask)  （self.gated(mask).size=(B,C,H,W)）
现在：
x= self.activation(x) * self.gct(mask)      (self.gct(mask).size=(B,C,1,1))
```

我认为L2范数会导致位置间的信息模糊，所以使用L1范数：

先冻结网络进行训练：

```python
for key, value in netG.named_parameters():
    if "gct" in key :
        continue
    else:
        value.requires_grad = False
```

冻结的bug:

<img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210823164722689.png" alt="image-20210823164722689" style="zoom:50%;" />

有含gct的层的梯度没有被冻结

<hr color=blue>


V3.1.2

```python
原来：
x = self.activation(x) * self.gated(mask)  （self.gated(mask).size=(B,C,H,W)）
V3.1.1：
x= self.activation(x) * self.gct(mask)      (self.gct(mask).size=(B,C,1,1))
V3.1.2：
x = self.activation(x) * self.gated(mask)*self.gct(mask)
```

1. **Non Local**：考虑了像素的特征，而没有考虑相对位置。倘若也将像素坐标位置 (x,y)考虑进来，其实就是全图版的 MeanShift Filter，一种经典的保边滤波算法。Nonlocal 还可以溯源到经典的 Non-local means 和 BM3D。其功效也来源于此，即高维 feature map 里存在大量的冗余信息，该类算法可以消除大量噪音。

在其中通过CBAM得到X Y 方向的加权。

![image-20210816162125730](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210816162125730.png)

3. 在self-attention中添加了放缩因子$\sqrt{d}$​

```python
energy=energy/(m_batchsize**0.5)

```

4. 添加了CA编码

```python
bise=self.CA(x)
out = self.gamma * out + x+bise
```

4. 把位置编码添加给self-attention

```python
bise=self.CA(x)
bise=bise.view(m_batchsize, -1, width * height)
proj_key=torch.add(proj_key,bise)
```

5. 加入MS_SSIM损失

```python
ms_ssim = MS_SSIM()
MS_loss = 1 - ms_ssim(img.cpu().detach(), ((complete_imgs + 1) * 255).cpu().detach())
whole_loss = g_loss + r_loss + MS_loss
```



### *V4：*输入用U2GE-net扣后的背景图，空白处比较少





|                  | <img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811200729171.png" alt="image-20210811200729171" style="zoom: 67%;" /> |                                                              |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| origin           | <img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811200709429.png" alt="image-20210811200709429" style="zoom: 67%;" /> | <img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811201227254.png" alt="image-20210811201227254" style="zoom: 67%;" /> |
| U2GE-net         | <img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811201156589.png" alt="image-20210811201156589" style="zoom: 67%;" /> | <img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811201520126.png" alt="image-20210811201520126" style="zoom: 67%;" /> |
| emmm….           | 有人的感觉                                                   | 添加一个膨胀试试                                             |
| cv2.dilate（10） | <img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811205403010.png" alt="image-20210811205403010" style="zoom: 67%;" /> | <img src="C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811205239009.png" alt="image-20210811205239009" style="zoom: 67%;" /> |



| origin                                                       | now                                                          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20210811210722890](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811210722890.png) | ![image-20210811210751351](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811210751351.png) |
| ![image-20210811210851373](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811210851373.png) | ![image-20210811210837884](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811210837884.png) |
| ![image-20210811211036553](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811211036553.png) | ![image-20210811211049133](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210811211049133.png) |

`Origin`

```python
        if self.detector is not None:
            bbox, body_mask = self.detector.inference(img[0])
            bg_mask = 1 - body_mask
        else:
            # bg is 1, ft is 0
            bg_mask = util.morph(src_info['cond'][:, -1:, :, :], ks=self._opt.bg_ks, code='erode')
            body_mask = 1 - bg_mask
src_info['bg'] = self.bgnet(img, masks=body_mask, only_x=True)
```

`Now`

```
"""U2GE-net"""
ft_mask_img=U2GE_net.net(src_path)
ft_mask_img[ft_mask_img<0.1]=0
ft_mask_img[ft_mask_img>=0.1]=1

"""膨胀"""
body_mask=ft_mask_img*255
body_mask=np.array(body_mask.cpu())
body_mask = cv2.dilate(body_mask, None, iterations=10)/255
body_mask=torch.from_numpy(body_mask)

ft_mask_img=body_mask.unsqueeze(0).cuda()
src_info['bg'] = self.bgnet(img, masks=ft_mask_img, only_x=True)
```



### *V5：*

自己搞数据集，重新练。

下载了Place2 256*256的数据集

### *V6:*

换网络：

基于无监督跨空间转换的多样化图像修复

[1].UCTGAN: Diverse Image Inpainting Based on Unsupervised Cross-Space Translation

作者 | Lei Zhao, Qihang Mo, Sihuan Lin, Zhizhong Wang, Zhiwen Zuo, Haibo Chen, Wei Xing, Dongming Lu

单位 | 浙江大学

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9CSmJSdndpYmVTVHRhOG52aWJnUXpZSlBMcEZEaWFDenhjbU9YOXpkdDNyaWEzOVFsOEV0UUpXVjVBb2R6NW9sOFhPclhZMnN5Q1NzQnpKem9XY3hQUTJKSlEvNjQw?x-oss-process=image/format,png)

用于超高分辨率图像修复的上下文残差聚合

[2].Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting

作者 | Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, Zhan Xu

单位 | 华为技术有限公司（加拿大）

代码 | https://github.com/Ascend-Huawei/Ascend-Canada/tree/master/Models/Research_HiFIll_Model

备注 | CVPR 2020 Oral 

[2].Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting

作者 | Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, Zhan Xu

单位 | 华为技术有限公司（加拿大）

代码 | https://github.com/Ascend-Huawei/Ascend-Canada/tree/master/Models/Research_HiFIll_Model

备注 | CVPR 2020 Oral 



![image-20210807214825971](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210807214825971.png)


图像修复的递归特征推理

[3].Recurrent Feature Reasoning for Image Inpainting

作者 | Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, Dacheng Tao

单位 | 武汉大学；悉尼大学

代码 | https://github.com/jingyuanli001/RFR-Inpainting

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9CSmJSdndpYmVTVHRhOG52aWJnUXpZSlBMcEZEaWFDenhjbXU5ZVF6S0dsOE1DY05PNmdXak5MdlVFM3hraWMzNGtpYm16SHNxR0RjaWFpY216SlRzTkEyRGdKMXcvNjQw?x-oss-process=image/format,png)

通过研究第一篇没有发现特别的，第二篇的对象是高分辨率的（能处理4K，用于训练的图片是512×512）果断pass，第三篇是基于大范围mask的，很符合这个网络，果断秒选。

### *V7:*用超分辨率

直接使用超分辨率对结果进行处理

### V8:修改训练的策略

由于这个修复网络针对的真的是图像修复，删除水印之类的东西。所以用于训练的mask蒙版多是随机生成为奇奇怪怪的条状：

![image-20210818195311604](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210818195311604.png)

但是， 对于我所需求的背景修复完全不同。故，我打算在训练的时候修改随机生成的mask策略，让其生成类似于扣去人后的图片的mask。

我们知道，人的**高度**是远大于**宽度**的。并且人一般在图片的中心，为了达到类似的效果，我采用了正态随机采样。

```python
mu, sigma = 0, 1
sampleNo = 2
s = np.random.normal(mu, sigma, sampleNo)/3*128
s=np.maximum(s,-s)
s[s>128]=128
H=int(s[0])
W=int(s[1])
mask[H:256-H,128-W:128+W]=1
```

为了提高网络的泛化性，添加少数的**涂鸦**。

![image-20210818213444754](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210818213444754.png)

![image-20210822144454004](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210822144454004.png)

| origin            | ![image-20210822212510439](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210822212510439.png) |
| ----------------- | ------------------------------------------------------------ |
| gan原本使用的模型 | ![image-20210822212215609](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210822212215609.png) |
| 9epoch            | ![image-20210822212807116](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210822212807116.png) |
| 10epoch           | ![image-20210822212447468](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210822212447468.png) |
| 11epoch           | ![image-20210822212708096](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210822212708096.png) |
| 12epoch           | ![image-20210822212735234](C:\Users\BlackFriday\AppData\Roaming\Typora\typora-user-images\image-20210822212735234.png) |



通过综合对比，最终选择了第9 epoch
